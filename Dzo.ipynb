{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:37: UserWarning: Unsupported `ReduceOp` for distributed computing.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AUDUSD-2000-2020-15m.csv', 'EURCHF-2000-2020-15m.csv', 'EURJPY-2000-2020-15m.csv', 'EURUSD-2000-2020-15m.csv', 'EUR_USD Historical Data.csv', 'USDCAD-2000-2020-15m.csv', 'USDCHF-2000-2020-15m.csv', 'USDJPY-2000-2020-15m.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"./datasets\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArrayDataset(Dataset):\n",
    "    def __init__(self, datasets):\n",
    "        super(ArrayDataset, self).__init__()\n",
    "        self._length = len(datasets[0])\n",
    "        for i, data in enumerate(datasets):\n",
    "            assert len(data) == self._length, \\\n",
    "                \"All arrays must have the same length; \\\n",
    "                array[0] has length %d while array[%d] has length %d.\" \\\n",
    "                % (self._length, i+1, len(data))\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return tuple(torch.from_numpy(data[idx]).float() \\\n",
    "                     for data in self.datasets)\n",
    "    \n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FXDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size, length, source_len, target_len, step):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.length = length\n",
    "        self.source_len = source_len\n",
    "        self.target_len = target_len\n",
    "        self.step = step\n",
    "\n",
    "    def split_sequence(self, source, target, source_len, target_len, step, target_start_next):\n",
    "        \"\"\" Split sequence with sliding window into\n",
    "            sequences of context features and target.\n",
    "            Args:\n",
    "                source (np.array): Source sequence\n",
    "                target (np.array): Target sequence\n",
    "                source_len (int): Length of input sequence.\n",
    "                target_len (int): Length of target sequence.\n",
    "                target_start_next (bool): If True, target sequence\n",
    "                        starts on the next time step of last step of source\n",
    "                        sequence. If False, target sequence starts at the\n",
    "                        same time step of source sequence.\n",
    "            Return:\n",
    "                X (np.array): sequence of features\n",
    "                y (np.array): sequence of targets\n",
    "        \"\"\"\n",
    "        assert len(source) == len(target), \\\n",
    "                'Source sequence and target sequence should have the same length.'\n",
    "\n",
    "        X, y = list(), list()\n",
    "        if not target_start_next:\n",
    "            target = np.vstack((np.zeros(target.shape[1], dtype=target.dtype), target))\n",
    "        for i in range(0, len(source), step):\n",
    "            # Find the end of this pattern:\n",
    "            src_end = i + source_len\n",
    "            tgt_end = src_end + target_len\n",
    "            # Check if beyond the length of sequence:\n",
    "            if tgt_end > len(target):\n",
    "                break\n",
    "            # Split sequences:\n",
    "            X.append(source[i:src_end, :])\n",
    "            y.append(target[src_end:tgt_end, :])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        df = pd.read_csv(self.data_dir, parse_dates=['DATE_TIME'])\n",
    "        self.data = df.iloc[:,1:].values\n",
    "        self.src, self.tgt = self.split_sequence(\n",
    "                self.data,\n",
    "                self.data,\n",
    "                self.source_len,\n",
    "                self.target_len,\n",
    "                self.step,\n",
    "                True\n",
    "        )\n",
    "\n",
    "    def setup(self):\n",
    "        # Split data into training set and test set :\n",
    "        test_idx = int(len(self.src) * 0.7)\n",
    "        src_train, src_test, tgt_train, tgt_test \\\n",
    "            = self.src[:test_idx], self.src[test_idx:], self.tgt[:test_idx], self.tgt[test_idx:]\n",
    "        # Split training data into train set and validation set:\n",
    "        src_train, src_val, tgt_train, tgt_val \\\n",
    "            = train_test_split(src_train, tgt_train, test_size=0.25, random_state=1)\n",
    "        # Prepare datasets\n",
    "        self.trainset = ArrayDataset([src_train, tgt_train])\n",
    "        self.valset = ArrayDataset([src_val, tgt_val])\n",
    "        self.testset = ArrayDataset([src_test, tgt_test])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        self.trainloader = DataLoader(\n",
    "                self.trainset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True\n",
    "        )\n",
    "        return self.trainloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        self.valloader = DataLoader(\n",
    "                self.valset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False\n",
    "        )\n",
    "        return self.valloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        self.testloader = DataLoader(\n",
    "                self.testset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False\n",
    "        )\n",
    "        return self.testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FXModule(pl.LightningModule):\n",
    "    def __init__(self,input_size=1, hidden_size=100, output_size=1):\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden_cell = (torch.zeros(1,1,self.hidden_size),\n",
    "                            torch.zeros(1,1,self.hidden_size))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq) ,1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 192, 4]) torch.Size([256, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "# Parse arguments:\n",
    "parser = ArgumentParser()\n",
    "parser = pl.Trainer.add_argparse_args(parser)\n",
    "parser.add_argument(\"--batch_size\", default=32)\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float)\n",
    "parser.add_argument(\"--cuda\", default=False)\n",
    "\n",
    "\"\"\"\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Model & data module:\n",
    "detector = PropagandaDetector(hparams=args)\n",
    "prop_dm = PropagandaDataModule()\n",
    "\n",
    "# Train & valid:\n",
    "trainer = pl.Trainer.from_argparse_args(args, fast_dev_run=True)\n",
    "trainer.fit(detector, prop_dm)\n",
    "\"\"\"\n",
    "\n",
    "fx_dm = FXDataModule(\n",
    "        data_dir=\"./datasets/EURUSD-2000-2020-15m.csv\",\n",
    "        batch_size=256,\n",
    "        length=100,\n",
    "        source_len=192,\n",
    "        target_len=4,\n",
    "        step=4\n",
    ")\n",
    "\n",
    "fx = FXModule(\n",
    "    source_size = 4\n",
    "    target_size = 4\n",
    "    hidden_size = 256\n",
    "    num_layers = 1\n",
    "    bidirectional=False,\n",
    "    dropout=0\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "fx_dm.prepare_data()\n",
    "fx_dm.setup()\n",
    "cac, lon = next(iter(fx_dm.train_dataloader()))\n",
    "print(cac.shape, lon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
